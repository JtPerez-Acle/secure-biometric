{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mediapipe opencv-python numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O detector.tflite -q https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Mediapipe Face Mesh utilities\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Drawing specs\n",
    "drawing_spec_landmark = mp_drawing.DrawingSpec(thickness=1, circle_radius=1, color=(0, 255, 0))\n",
    "drawing_spec_connection = mp_drawing.DrawingSpec(thickness=1, color=(255, 0, 0))\n",
    "\n",
    "def extract_face_landmark_points(image, face_landmarks):\n",
    "    \"\"\"\n",
    "    Convert normalized MediaPipe FaceMesh landmark coordinates into\n",
    "    pixel coordinates (x, y, z*) for this face.\n",
    "    z is the normalized depth value (negative = out of the screen).\n",
    "    \"\"\"\n",
    "    h, w, _ = image.shape\n",
    "    landmark_points = []\n",
    "    for landmark in face_landmarks.landmark:\n",
    "        x_px = int(landmark.x * w)\n",
    "        y_px = int(landmark.y * h)\n",
    "        z_val = landmark.z  # Normalized depth\n",
    "        landmark_points.append([x_px, y_px, z_val])\n",
    "    return np.array(landmark_points, dtype=np.float32)\n",
    "\n",
    "\n",
    "def calculate_distances_and_angles(landmarks_array):\n",
    "    \"\"\"\n",
    "    Compute example distances (e.g., between mouth corners) and angles (e.g., eyebrow slopes).\n",
    "    Return as a dictionary for further analysis. You can add as many metrics as you like.\n",
    "    \n",
    "    For reference landmark indices (468 total):\n",
    "    https://github.com/tensorflow/tfjs-models/tree/master/face-landmarks-detection#keypoints\n",
    "    \n",
    "    We'll pick a few for demonstration:\n",
    "      - 61 = Right mouth corner\n",
    "      - 291 = Left mouth corner\n",
    "      - 159 = Right eyebrow top (approx)\n",
    "      - 386 = Left eyebrow top (approx)\n",
    "    Adjust indices as needed for your analysis.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Ensure we have enough landmarks\n",
    "    if landmarks_array.shape[0] < 400:\n",
    "        return metrics  # Not enough landmarks, return empty\n",
    "    \n",
    "    # Example: Distance between mouth corners (Euclidean distance)\n",
    "    # Right mouth corner = index 61, Left mouth corner = index 291\n",
    "    mouth_right = landmarks_array[61][:2]   # (x, y)\n",
    "    mouth_left  = landmarks_array[291][:2]\n",
    "    mouth_width = np.linalg.norm(mouth_right - mouth_left)\n",
    "    metrics[\"mouth_width\"] = mouth_width\n",
    "    \n",
    "    # Example: Eyebrow slope (difference in Y between left and right eyebrow points)\n",
    "    # Right eyebrow top = index 159, Left eyebrow top = index 386\n",
    "    brow_right = landmarks_array[159][:2]\n",
    "    brow_left  = landmarks_array[386][:2]\n",
    "    brow_slope = brow_right[1] - brow_left[1]\n",
    "    metrics[\"eyebrow_slope\"] = brow_slope\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_head_pose(image, landmarks_array):\n",
    "    \"\"\"\n",
    "    Estimate head pose (pitch, yaw, roll) using solvePnP with a minimal subset\n",
    "    of 3D points. We map 2D landmarks to a rough 3D head model.\n",
    "    \n",
    "    This is an approximation. A refined approach would require a calibrated\n",
    "    camera, better 3D reference points, and possibly more advanced fitting.\n",
    "    \"\"\"\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    # For simplicity, letâ€™s pick 6 standard face landmarks\n",
    "    # Indices refer to MediaPipe face mesh:\n",
    "    #   33 = Right eye outer corner\n",
    "    #  263 = Left eye outer corner\n",
    "    #   61 = Right mouth corner\n",
    "    #  291 = Left mouth corner\n",
    "    #   1  = Nose tip\n",
    "    #  199 = Chin\n",
    "    # You can refine or choose a different set of landmarks.\n",
    "    landmark_indices = [33, 263, 61, 291, 1, 199]\n",
    "\n",
    "    # 3D model reference points in a rough model-based coordinate system (e.g., an average face)\n",
    "    # The values here are approximate and serve as a starting template.\n",
    "    # Adjust or refine for better accuracy.\n",
    "    model_3d_points = np.array([\n",
    "        [ 0.0,    0.0,    0.0   ],  # Nose tip\n",
    "        [-30.0,   -65.0,  -20.0 ],  # Chin\n",
    "        [-60.0,    0.0,   -30.0 ],  # Left eye outer corner\n",
    "        [ 60.0,    0.0,   -30.0 ],  # Right eye outer corner\n",
    "        [-40.0,   -30.0,  -30.0 ],  # Left mouth corner\n",
    "        [ 40.0,   -30.0,  -30.0 ],  # Right mouth corner\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # We need to reorder the model points to match the chosen indices\n",
    "    # We'll assume the order of our indices matches the order in model_3d_points above\n",
    "    # That means: [1 (nose), 199 (chin), 263 (L eye), 33 (R eye), 291 (L mouth), 61 (R mouth)]\n",
    "    # but we picked them in a different order, so we rearrange them carefully.\n",
    "    # Actually, let's define an order that matches our indices array:\n",
    "    # landmark_indices = [33, 263, 61, 291, 1, 199]\n",
    "    # We'll map:\n",
    "    #   1   -> model_3d_points[0] (nose tip)\n",
    "    #   199 -> model_3d_points[1] (chin)\n",
    "    #   263 -> model_3d_points[2] (left eye)\n",
    "    #   33  -> model_3d_points[3] (right eye)\n",
    "    #   291 -> model_3d_points[4] (left mouth)\n",
    "    #   61  -> model_3d_points[5] (right mouth)\n",
    "    # Adjust accordingly:\n",
    "\n",
    "    # Extract 2D coordinates from the landmarks_array\n",
    "    # We'll reorder them to align with model_3d_points\n",
    "    nose_2d  = landmarks_array[1][:2]\n",
    "    chin_2d  = landmarks_array[199][:2]\n",
    "    leye_2d  = landmarks_array[263][:2]\n",
    "    reye_2d  = landmarks_array[33][:2]\n",
    "    lmouth_2d= landmarks_array[291][:2]\n",
    "    rmouth_2d= landmarks_array[61][:2]\n",
    "\n",
    "    image_points = np.array([\n",
    "        nose_2d,   # nose tip\n",
    "        chin_2d,   # chin\n",
    "        leye_2d,   # left eye outer corner\n",
    "        reye_2d,   # right eye outer corner\n",
    "        lmouth_2d, # left mouth corner\n",
    "        rmouth_2d, # right mouth corner\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Camera internals (approx for a typical webcam)\n",
    "    focal_length = w\n",
    "    center = (w / 2, h / 2)\n",
    "    camera_matrix = np.array([\n",
    "        [focal_length, 0,             center[0]],\n",
    "        [0,            focal_length,  center[1]],\n",
    "        [0,            0,             1       ]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Assume no lens distortion\n",
    "    dist_coeffs = np.zeros((4,1))\n",
    "\n",
    "    # Solve the PnP to get rotation/translation\n",
    "    # Make sure our 3D model points align with the order of the 2D image_points\n",
    "    # model_3d_points must match the same semantic meaning\n",
    "    # For demonstration, reorder our model_3d_points accordingly:\n",
    "    #   index: semantic\n",
    "    #     0 -> nose\n",
    "    #     1 -> chin\n",
    "    #     2 -> left eye\n",
    "    #     3 -> right eye\n",
    "    #     4 -> left mouth\n",
    "    #     5 -> right mouth\n",
    "    # So we reorder model_3d_points in the same semantic order:\n",
    "    model_3d_points_ordered = np.array([\n",
    "        [ 0.0,    0.0,    0.0   ],  # nose tip\n",
    "        [-30.0,   -65.0,  -20.0 ],  # chin\n",
    "        [-60.0,    0.0,   -30.0 ],  # left eye\n",
    "        [ 60.0,    0.0,   -30.0 ],  # right eye\n",
    "        [-40.0,   -30.0,  -30.0 ],  # left mouth\n",
    "        [ 40.0,   -30.0,  -30.0 ],  # right mouth\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "        model_3d_points_ordered,\n",
    "        image_points,\n",
    "        camera_matrix,\n",
    "        dist_coeffs,\n",
    "        flags=cv2.SOLVEPNP_ITERATIVE\n",
    "    )\n",
    "\n",
    "    # Convert rotation vector to Euler angles (pitch, yaw, roll)\n",
    "    # The rotation matrix can be converted to angles\n",
    "    rmat, _ = cv2.Rodrigues(rotation_vector)\n",
    "    # From the rotation matrix, compute euler angles\n",
    "    # Reference: https://docs.opencv.org/4.x/dc/d84/group__core__mat.html#ga3c277fa1f4e5f5dd2c7f48adab2b7f72\n",
    "    # We'll assume X=Pitch, Y=Yaw, Z=Roll with one of the common CV conventions\n",
    "    sy = math.sqrt(rmat[0,0]*rmat[0,0] + rmat[1,0]*rmat[1,0])\n",
    "    pitch = math.degrees(math.atan2(rmat[2,1], rmat[2,2]))\n",
    "    yaw   = math.degrees(math.atan2(-rmat[2,0], sy))\n",
    "    roll  = math.degrees(math.atan2(rmat[1,0], rmat[0,0]))\n",
    "\n",
    "    head_pose = {\n",
    "        \"pitch\": pitch,\n",
    "        \"yaw\": yaw,\n",
    "        \"roll\": roll\n",
    "    }\n",
    "\n",
    "    return head_pose\n",
    "\n",
    "\n",
    "def calculate_average_facial_color(image, landmarks_array):\n",
    "    \"\"\"\n",
    "    Compute the average color in a bounding polygon around the face or\n",
    "    a bounding rectangle. For simplicity, let's use a bounding rectangle\n",
    "    from the min/max landmark coordinates. This is a placeholder approach.\n",
    "    \"\"\"\n",
    "    x_coords = landmarks_array[:, 0]\n",
    "    y_coords = landmarks_array[:, 1]\n",
    "\n",
    "    # Get bounding box\n",
    "    x_min, x_max = int(np.min(x_coords)), int(np.max(x_coords))\n",
    "    y_min, y_max = int(np.min(y_coords)), int(np.max(y_coords))\n",
    "\n",
    "    # Clip to image boundaries\n",
    "    h, w, _ = image.shape\n",
    "    x_min = max(0, x_min)\n",
    "    x_max = min(w - 1, x_max)\n",
    "    y_min = max(0, y_min)\n",
    "    y_max = min(h - 1, y_max)\n",
    "\n",
    "    face_roi = image[y_min:y_max, x_min:x_max]\n",
    "    if face_roi.size == 0:\n",
    "        return (0, 0, 0)\n",
    "\n",
    "    # Calculate mean color (B, G, R) in face region\n",
    "    mean_color = cv2.mean(face_roi)[:3]  # ignoring alpha if present\n",
    "    # mean_color is in BGR, e.g. (B, G, R)\n",
    "    # Convert to a nicer format or keep it as is\n",
    "    return mean_color\n",
    "\n",
    "\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=False,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                continue\n",
    "\n",
    "            # Convert to RGB for MediaPipe\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(frame_rgb)\n",
    "\n",
    "            if results.multi_face_landmarks:\n",
    "                for face_landmarks in results.multi_face_landmarks:\n",
    "                    # Draw the face mesh\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=frame,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                        landmark_drawing_spec=drawing_spec_landmark,\n",
    "                        connection_drawing_spec=drawing_spec_connection\n",
    "                    )\n",
    "\n",
    "                    # Extract landmarks as array\n",
    "                    landmarks_array = extract_face_landmark_points(frame, face_landmarks)\n",
    "\n",
    "                    # 1) Calculate distances and angles between certain facial landmarks\n",
    "                    face_metrics = calculate_distances_and_angles(landmarks_array)\n",
    "\n",
    "                    # 2) Estimate head pose (pitch, yaw, roll)\n",
    "                    head_pose = calculate_head_pose(frame, landmarks_array)\n",
    "\n",
    "                    # 3) Calculate average facial color (rough approximation)\n",
    "                    avg_color = calculate_average_facial_color(frame, landmarks_array)\n",
    "\n",
    "                    # Print or log data (in real use, you'd store or analyze these values)\n",
    "                    # For demonstration, we print them in the console.\n",
    "                    print(\"Face Metrics:\", face_metrics)\n",
    "                    print(\"Head Pose:\", head_pose)\n",
    "                    print(\"Average Facial Color (BGR):\", avg_color)\n",
    "                    print(\"----\")\n",
    "\n",
    "            cv2.imshow(\"Enhanced Face Mesh Analysis\", frame)\n",
    "\n",
    "            # Press 'Esc' to exit\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secure-biometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
